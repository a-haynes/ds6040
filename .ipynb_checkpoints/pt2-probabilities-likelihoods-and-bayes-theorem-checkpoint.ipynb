{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# import matplotlib\nimport matplotlib.pyplot as plt\n# import seaborn\nimport seaborn as sns\n# settings for seaborn plotting style\nsns.set(color_codes=True)\n# settings for seaborn plot sizes\nsns.set(rc={'figure.figsize':(9.5,5)})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-09T00:51:42.470956Z","iopub.execute_input":"2022-02-09T00:51:42.471348Z","iopub.status.idle":"2022-02-09T00:51:42.479621Z","shell.execute_reply.started":"2022-02-09T00:51:42.471312Z","shell.execute_reply":"2022-02-09T00:51:42.478637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. In this section, we will compute some *probabilities* and *likelihoods*.","metadata":{}},{"cell_type":"markdown","source":"The binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent trials.\n\nA ***probability*** is a number assigned to a possible outcome, and the sum (or integral) of the probabilities of all possible outcomes in the support of the distrubution is equal to one.\n\nA ***likelihood*** is a number that is assigned to a hypothesis about the underlying paraemters for a distribution.  \n* For discrete random variables, the likelihood of the parameters $\\theta$ given the outcome $X$, written $L(\\theta|X)$, is equal to the probability mass function (PMF) for this outcome given these parameters, $L(\\theta|X)=P(X|\\theta)$. \n* For continuous random variables, the likelihood of the parameters $\\theta$ given the outcome $X$, written $L(\\theta|X)$, is equal to the probability desnity function (PDF) for this outcome given these parameters, $L(\\theta|X)=f(X|\\theta)$.\n\nIn summary, the likelihood is a function of the parameters (assuming the outcome is fixed) and the probability (mass/density) function is a funciton of the outcome, assuming the parameters for the distribution are fixed.  But, both are equal to the probability mass/density function for the given outcome and parameters.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import binom\n# set the parameters\nparam_n = 10 # number of trials\nparam_p = 0.3 # probability of success in a single trial\nk_value = 5 #NOTE: k = the number of successes\n\n# compute the probability/likelihood\np = binom.pmf(k=k_value, n=param_n, p=param_p) \n\nprint('For the discrete binomial distribution:')\nprint(f'L(n={param_n},p={param_p}|k={k_value}) = p(k={k_value}|n={param_n},p={param_p}) = {p}')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:51:46.350476Z","iopub.execute_input":"2022-02-09T00:51:46.350967Z","iopub.status.idle":"2022-02-09T00:51:46.360718Z","shell.execute_reply.started":"2022-02-09T00:51:46.350934Z","shell.execute_reply":"2022-02-09T00:51:46.359598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import norm\n# set the parameters\nparam_loc = 3 # mean\nparam_scale = 2 # standard deviation\nx_value = 5\n\n# compute the probability/likelihood\np = norm.pdf(x=x_value, loc=param_loc,scale=param_scale)\n\nprint('For the continuous gaussian distribution:')\nprint(f'L(\\u03BC={param_loc},\\u03C3={param_scale}|x={x_value}) = f(x={x_value}|\\u03BC={param_loc},\\u03C3={param_scale}) = {p}')\n# See https://gist.github.com/beniwohli/765262 for the unicodes for the greek alphabet ","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:51:49.86214Z","iopub.execute_input":"2022-02-09T00:51:49.862793Z","iopub.status.idle":"2022-02-09T00:51:49.873877Z","shell.execute_reply.started":"2022-02-09T00:51:49.862741Z","shell.execute_reply":"2022-02-09T00:51:49.872813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Using Bayes Theorem\n\nBayes theorem is:\n$$\nP(\\theta | X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)},\n$$\nwhere: \n* $P(\\theta | X)$ is the probability for the parameter is $\\theta$ given the observed outcom $X$.\n* $P(X|\\theta)$ the likelihood for the probability distribution.  It is standard to use the discrite case notation, in which this is the probability of getting the outcome $X$ if $\\theta$ is the underlying parameter for the distribution.\n* $P(\\theta)$ is the prior probability for $\\theta$\n* $P(X)$ is the probability of getting $X$ as an outcome.\n\nNote that if we there are only a finite number of possible values $\\{\\theta_1,\\theta_2,...,\\theta_N\\}$ for the parameter $\\theta$, then we can compute $P(X)$ by\n$$\nP(X) = P(X|\\theta_1)P(\\theta_1) + P(X|\\theta_2)P(\\theta_2) +\\cdots + P(X|\\theta_N)P(\\theta_N).\n$$\nThis is called the 'normalization' or 'conditioning' formula.  If there is a continuous set of possible $\\theta$ values, for example an interval $[a,b]$, this sum is replaced by an integral.","metadata":{}},{"cell_type":"markdown","source":"**QUESTION 1:** Suppose that we have a coin that is either fair (p=0.5 where p is the probaiblity of getting heads) or unfiar with p=0.1.  We have flipped a coin 50 times and got heads 21 times.  What is the probability that this coin is fair?","metadata":{}},{"cell_type":"markdown","source":"**ANSWER**:  Notice that this question is really asking about the underlying parameter for the probability distribution for the coin flip: is p=0.5 or is p=0.1?  So we are going to use Bayes theorem to compute these two probabilities.\n\nTo start, we are going to be using the binomial probability distribution - for given parameter values for $n$ flips of a coin that has probability $p$ of getting heads, the probability probability of getting $k$ heads is:\n$$\nP(k|n,p)={n \\choose k}p^k(1-p)^{n-k}.\n$$\n\nFor our given problem:\n* In Bayes Theorem, $\\theta$ represents the parameter(s) for the distribution, which in this case is $n=50, p=0.5$ or $n=50, p=0.1$.\n* In Bayes Theorem, $X$ represented the observed outcome, which in this case is $k=21$ heads out of our coin flips.\n* We are not told any information about the prior probability, we are going to assume the possible parameters have the same prio probability, $P(n=50, p=0.5)=P(n=50, p=0.1)=0.5$.\n\nBayes Theorem is then:\n$$\nP(n=50, p=0.5|k=21)=\\frac{P(k=21|n=50, p=0.5)P(n=50, p=0.5)}{P(k=21)}.\n$$\n\nNow, using the fact that the prior probability is 0.5 and using the normlization formula:\n$$\nP(n=50, p=0.5|k=21)=\\frac{P(k=21|n=50, p=0.5)0.5}{P(k=21|n=50, p=0.1)0.5+P(k=21|n=50, p=0.5)0.5}.\n$$\n\nSo all we really nedd to compute are the likelihoods $P(k=21|n=50, p=0.1)$ and $P(k=21|n=50, p=0.5)$.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import binom\n\n# compute the likelihoods\nk_value, param_n, param_p = 21, 50, 0.5,\nL1 = binom.pmf(k=k_value, n=param_n, p=param_p) \nprint(f'L(n={param_n},p={param_p}|k={k_value}) = p(k={k_value}|n={param_n},p={param_p}) = {L1}')\nk_value, param_n, param_p = 21, 50, 0.1 \nL2 = binom.pmf(k=k_value, n=param_n, p=param_p) \nprint(f'L(n={param_n},p={param_p}|k={k_value}) = p(k={k_value}|n={param_n},p={param_p}) = {L2}')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:00.414045Z","iopub.execute_input":"2022-02-09T00:52:00.414744Z","iopub.status.idle":"2022-02-09T00:52:00.426942Z","shell.execute_reply.started":"2022-02-09T00:52:00.414679Z","shell.execute_reply":"2022-02-09T00:52:00.425643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can input these into our formula above to get the probability that the coin is fair.","metadata":{}},{"cell_type":"code","source":"N1 = L1*0.5\nprint(f'Numerator for p=0.5: {N1}')\nN2 = L2*0.5\nprint(f'Numerator for p=0.1: {N2}')\nD = L1*0.5 + L2*0.5\nprint(f'Denominator: {D}')\nprint(f'Probability that the coin is fair = P(n=50,p=0.5|k=21) = {N1/D}')\nprint(f'Probability that the coin is not fair with p=0.1 = P(n=50,p=0.1|k=21) = {N2/D}')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:04.142315Z","iopub.execute_input":"2022-02-09T00:52:04.142681Z","iopub.status.idle":"2022-02-09T00:52:04.149381Z","shell.execute_reply.started":"2022-02-09T00:52:04.142651Z","shell.execute_reply":"2022-02-09T00:52:04.148562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**QUESTION 2:** Suppose that we have flipped a coin 50 times and got heads 21 times. Estimate the probability distribution for the probability $p$ for getting heads of a single coin toss.  Do this first by assuming that $p$ has one of the values $ùëù=0,ùëù=0.01,ùëù=0.02,...,ùëù=0.98,ùëù=0.99,ùëù=1$ and has a discrete distribution, then use this to approximate the PDF for a continuous distribution for $p$.","metadata":{}},{"cell_type":"markdown","source":"**ANSWER:** We are going to use the same formulas as above, but for $p=0,p=0.01,p=0.02,...,p=0.98,p=0.99,p=1$.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import binom\n\n# create an array of possible values for p\nx = np.arange(0, 1, 0.01)\nprint(f'x = {x}')\n\n# compute the likelihoods for each of these\nL = binom.pmf(k=21, n=50, p=x)\nprint(f'L = {L}')\n\n# compute the denominator in Bayes Theorem (i.e. the normalizing factor)\nprior_prob = 1/len(L)\nD = np.sum(L*prior_prob)\nprint(f'D = {D}')\n\n# now compute the probability for each x-vaue using Bayes Theorem\nP= L*prior_prob / D\nprint(f'P={P}')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:07.311708Z","iopub.execute_input":"2022-02-09T00:52:07.312311Z","iopub.status.idle":"2022-02-09T00:52:07.326871Z","shell.execute_reply.started":"2022-02-09T00:52:07.312278Z","shell.execute_reply":"2022-02-09T00:52:07.32602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nax = sns.scatterplot(x, P)\nax.set(xlabel='x', ylabel='P(p=x)', title=f'Posterior Probability Mass Function for p (discrete distribution, every 0.01 points)');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:11.255233Z","iopub.execute_input":"2022-02-09T00:52:11.255578Z","iopub.status.idle":"2022-02-09T00:52:11.630481Z","shell.execute_reply.started":"2022-02-09T00:52:11.25555Z","shell.execute_reply":"2022-02-09T00:52:11.6296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that we use filled circle markers in this plot because this represents a discte distribution.  The sum of the all of the values is 1.","metadata":{}},{"cell_type":"markdown","source":"To compute the PDF for the continuous case, we will change the normalizing/conditioning constant to an integral:\n$$\nP(X)=\\int_0^1 P(\\theta|X)P(\\theta)d\\theta \\approx \\sum_i P(\\theta_i|X) P(\\theta_i)\\Delta \\theta\n$$\n","metadata":{}},{"cell_type":"code","source":"# compute the denominator in Bayes Theorem (i.e. the normalizing factor) approximating the integral\nprior_prob = 1/len(L)\ndelta_theta = 0.01\nD = np.sum(L*prior_prob*delta_theta)\nprint(f'D = {D}')\n\n# now compute the probability for each x-value using Bayes Theorem\nP= L*prior_prob / D\nprint(f'P={P}')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:15.68718Z","iopub.execute_input":"2022-02-09T00:52:15.687524Z","iopub.status.idle":"2022-02-09T00:52:15.69583Z","shell.execute_reply.started":"2022-02-09T00:52:15.687495Z","shell.execute_reply":"2022-02-09T00:52:15.695021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.lineplot(x, P)\nax.set(xlabel='x', ylabel='f(x)', title=f'Probability Density Function for p (continuous distribution)');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:17.807818Z","iopub.execute_input":"2022-02-09T00:52:17.808496Z","iopub.status.idle":"2022-02-09T00:52:18.113501Z","shell.execute_reply.started":"2022-02-09T00:52:17.808456Z","shell.execute_reply":"2022-02-09T00:52:18.112341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This cruve represents a continuous probability distribution.  Its integral, equal to the area under the curve, is 1. (or approximately 1, since this plot is a numerical approximation...)","metadata":{}},{"cell_type":"markdown","source":"# 4. CHALLENGE QUESTION - ANSWER\n\nHere is a challenge question if you want to test your understanding, and see if you can extend the use of Bayes Theorem above to a 2D distribution.\n\nSuppose you have a single observation $x=4.2$ and you believe it was generated by a normal distribution.  Estimate the probability distribution for the parameters $\\mu$ and $\\sigma$ of the normal distribution which generated $x=4.2$. (Since we are estimating a distribution for $\\mu$ and $\\sigma$, this is called a joint distribution, but the principles are the same, just in 2-dimensions.)  \n\nDo this first by computing $P(\\mu=x, \\sigma=y)$ for all values of $(x,y)$ in a grid with $x$ in the interval $[-10,10]$ and y in the interval $[0,10]$ with a stepsize of $0.1$.   Assume that the distribution is continuous, so you have to normalize/condition by an integral.  The Python Code for generating the X and Y values is provided below along with some plotting code.  You just have to compute the formula for $Z = P(\\mu=x, \\sigma=y)$.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import norm\n\n# Create the Y,Y grid\ndelta_x = 0.1\ndelta_y = 0.1\nX, Y = np.meshgrid(np.arange(2.5, 7.5, delta_x), np.arange(0.1, 2, delta_y))\n# X, Y = np.meshgrid(np.arange(3, 5, 0.01), np.arange(0.1, 2, 0.01))\n\n# compute the likelihoods for each of these\n\n# compute the probability/likelihood\nL = norm.pdf(x=4.2, loc=X, scale=Y)\nprint(f'L = {L}')\n\n# compute the denominator in Bayes Theorem (i.e. the normalizing factor)\nprior_prob = 1/len(L)\nD = np.sum(L*(delta_x*delta_y)*prior_prob)\nprint(f'D = {D}')\n\n# now compute the probability for each x-vaue using Bayes Theorem\nP= L*prior_prob / D\nprint(f'P={P}')\n\n\nZ = P","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:22.670617Z","iopub.execute_input":"2022-02-09T00:52:22.671001Z","iopub.status.idle":"2022-02-09T00:52:22.713069Z","shell.execute_reply.started":"2022-02-09T00:52:22.670969Z","shell.execute_reply":"2022-02-09T00:52:22.711869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.contour(X, Y, Z, 20, cmap='twilight_shifted');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:31.24726Z","iopub.execute_input":"2022-02-09T00:52:31.247639Z","iopub.status.idle":"2022-02-09T00:52:31.565038Z","shell.execute_reply.started":"2022-02-09T00:52:31.24761Z","shell.execute_reply":"2022-02-09T00:52:31.564177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.contourf(X, Y, Z, 20, cmap='RdGy')\nplt.colorbar();","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:34.157694Z","iopub.execute_input":"2022-02-09T00:52:34.158084Z","iopub.status.idle":"2022-02-09T00:52:34.518957Z","shell.execute_reply.started":"2022-02-09T00:52:34.158027Z","shell.execute_reply":"2022-02-09T00:52:34.51814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contours = plt.contour(X, Y, Z, 3, colors='black')\nplt.clabel(contours, inline=True, fontsize=8)\n\nplt.imshow(Z, extent=[np.min(X), np.max(X), np.min(Y), np.max(Y)], origin='lower',\n           cmap='RdGy', alpha=0.5)\nplt.colorbar();","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:36.958754Z","iopub.execute_input":"2022-02-09T00:52:36.959297Z","iopub.status.idle":"2022-02-09T00:52:37.398487Z","shell.execute_reply.started":"2022-02-09T00:52:36.959259Z","shell.execute_reply":"2022-02-09T00:52:37.397427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.axes(projection='3d')\nax.plot_wireframe(X, Y, Z, color='r');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:40.095012Z","iopub.execute_input":"2022-02-09T00:52:40.095397Z","iopub.status.idle":"2022-02-09T00:52:40.252327Z","shell.execute_reply.started":"2022-02-09T00:52:40.095367Z","shell.execute_reply":"2022-02-09T00:52:40.250998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.axes(projection='3d');\nax.plot_surface(X, Y, Z, cmap='jet');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:42.103224Z","iopub.execute_input":"2022-02-09T00:52:42.103609Z","iopub.status.idle":"2022-02-09T00:52:42.357969Z","shell.execute_reply.started":"2022-02-09T00:52:42.103574Z","shell.execute_reply":"2022-02-09T00:52:42.356928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import cm# Normalize the colors based on Z value\nnorm = plt.Normalize(Z.min(), Z.max())\ncolors = cm.jet(norm(Z))\nax = plt.axes(projection='3d')\nsurf = ax.plot_surface(X, Y, Z, facecolors=colors, shade=False)\nsurf.set_facecolor((0,0,0,0))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:44.44667Z","iopub.execute_input":"2022-02-09T00:52:44.447016Z","iopub.status.idle":"2022-02-09T00:52:44.925436Z","shell.execute_reply.started":"2022-02-09T00:52:44.446986Z","shell.execute_reply":"2022-02-09T00:52:44.924297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.axes(projection='3d')\nax.contour3D(X, Y, Z, 55, cmap='twilight_shifted');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:46.973317Z","iopub.execute_input":"2022-02-09T00:52:46.973796Z","iopub.status.idle":"2022-02-09T00:52:47.17542Z","shell.execute_reply.started":"2022-02-09T00:52:46.973764Z","shell.execute_reply":"2022-02-09T00:52:47.174375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. PRIORS\n\nIn this section we will present common methods for including a prior probability distribtuion.","metadata":{}},{"cell_type":"markdown","source":"**5. 1 UNINFORMATIVE PRIOR** One option is to choose a prior probability distribution that adds very little information.  This was what we used in the previous example, repeated here (without the test output).  The prior is constant, which is a common choice when the domain for the parameters is either finite (for discrete distributions) or finite lenght (for continuous distributions).","metadata":{}},{"cell_type":"code","source":"# create an array of possible values for p\nx = np.arange(0, 1, 0.01)\n\n# compute the likelihoods for each of these\nL = binom.pmf(k=21, n=50, p=x)\n\n# compute the denominator in Bayes Theorem (i.e. the normalizing factor) approximating the integral\nprior_prob = 1/len(L)\ndelta_theta = 0.01 # could be x[1] - x[0]\nD = np.sum(L*prior_prob*delta_theta)\n\n# now compute the probability for each x-value using Bayes Theorem\nP= L*prior_prob / D\n\nax = sns.lineplot(x, P)\nax.set(xlabel='x', ylabel='f(x)', title=f'Probability Density Function for p (constant prior)');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:52:52.906298Z","iopub.execute_input":"2022-02-09T00:52:52.906672Z","iopub.status.idle":"2022-02-09T00:52:53.213825Z","shell.execute_reply.started":"2022-02-09T00:52:52.906634Z","shell.execute_reply":"2022-02-09T00:52:53.212803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5.2 EXPERT KNOWLEDGE PRIORS:** If an expert has some knowledge about that are likely values of the parameters, they can be used to create a prior.  This can be useful, but also very subjective.\n\nSuppose we want to use our intuition that most coins are fair.  We could assume a prior probabiltiy for p that is normal with mean of 0.5 and a very small standard deviation:","metadata":{}},{"cell_type":"code","source":"from scipy.stats import norm\n\nparam_mean_for_prior = 0.5\nparam_stdv_for_prior = 0.05 #Essentially a lower stdev is a higher certainty \n\nprior_prob_expert = norm.pdf(x=x, loc=param_mean_for_prior, scale=param_stdv_for_prior)\n\n# plot the prior distribution\nax = sns.lineplot(x, prior_prob_expert)\nax.set(xlabel='x', ylabel='P(p=x)', title=f'Prior Probability Distribution Expecting A Fair Coin');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:59:51.894789Z","iopub.execute_input":"2022-02-09T00:59:51.895339Z","iopub.status.idle":"2022-02-09T00:59:52.2106Z","shell.execute_reply.started":"2022-02-09T00:59:51.895302Z","shell.execute_reply":"2022-02-09T00:59:52.209661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can compute the posterior probability distribution using this prior.  We plot this with the distribution using the flat prior for comparison.","metadata":{}},{"cell_type":"code","source":"D = np.sum(L*prior_prob_expert*delta_theta)\n\n# now compute the probability for each x-value using Bayes Theorem\nP_expert_prior= L*prior_prob_expert / D\n\nax = sns.lineplot(x, P)\nax = sns.lineplot(x, P_expert_prior)\nax.set(xlabel='x', ylabel='f(x)', title=f'Probability Density Function for p (continuous distribution)');\nplt.legend(labels=['P, Flat Prior', 'P, Expert Prior']);\n\n#Expert input adjusts the mean and tightens confidence","metadata":{"execution":{"iopub.status.busy":"2022-02-09T00:59:54.253834Z","iopub.execute_input":"2022-02-09T00:59:54.254372Z","iopub.status.idle":"2022-02-09T00:59:54.60592Z","shell.execute_reply.started":"2022-02-09T00:59:54.254336Z","shell.execute_reply":"2022-02-09T00:59:54.604995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that using an expert prior shifted the posterior probability toward a fair distribution.\nQUESTION: What would you expect if:\n1. The standard deviation on the prior were smaller?\n2. There were more tirals with the same proportion of success in the initial data (i.e. n=500, k=210)?\n\nSuggestion: Mofidy the previous code to see if your answers are correct.","metadata":{}},{"cell_type":"markdown","source":"**5.3 Conjugate Priors** Conjugate priors are priors that have a functional form that is works well with the form for the likelihood function to make computation of the posterior probability much easier.  The word 'conjugate' means coupled, or paired.\n\nIn practice, usually we are presented with a situation that determines the form for the likelihood function, and we have a choice of what prior to use.  Choosing a conjugate prior allows us to find an analytic formula for the posterior probability instead of the numerical integral methods using in Sections 5.1 and 5.2","metadata":{}},{"cell_type":"markdown","source":"EXAMPLE: Binomial likelihood with Beta prior yeilds a Beta posterior.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import beta\n\n# create our beta prior\nparam_prior_a = 1\nparam_prior_b = 20\nprior_prob = beta.pdf(x=x, a=param_prior_a, b=param_prior_b)\n\n#An alpha = beta = 1 is an uninformative prior, alpha = beta < 1 is a bathtub distribution\n\n# plot the prior distribution\nax = sns.lineplot(x, prior_prob)\nax.set(xlabel='x', ylabel='P(p=x)', title=f'Beta Prior Probability Distribution with a={param_prior_a} and b={param_prior_b}');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T01:03:17.944726Z","iopub.execute_input":"2022-02-09T01:03:17.945074Z","iopub.status.idle":"2022-02-09T01:03:18.250825Z","shell.execute_reply.started":"2022-02-09T01:03:17.945028Z","shell.execute_reply":"2022-02-09T01:03:18.250028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import binom\nfrom scipy.stats import beta\n\n# create an array of possible values for p\nx = np.arange(0, 1, 0.01)\n\n# compute the likelihoods for each of these\nparam_k = 21\nparam_n = 50\nL = binom.pmf(k=param_k, n=param_n, p=x)\n\n# compute the denominator in Bayes Theorem (i.e. the normalizing factor) approximating the integral\ndelta_theta = 0.01\nD_beta_prior = np.sum(L*prior_prob*delta_theta)\n\n# now compute the probability for each x-value using Bayes Theorem\nP_beta_prior= L*prior_prob / D_beta_prior\n\nax = sns.lineplot(x, P)\nax = sns.lineplot(x, P_beta_prior)\nax.set(xlabel='x', ylabel='f(x)', title=f'Probability Density Function for p (continuous distribution)');\nplt.legend(labels=['P, Flat Prior', 'P, Conjugate Beta Prior']);","metadata":{"execution":{"iopub.status.busy":"2022-02-09T01:03:26.457102Z","iopub.execute_input":"2022-02-09T01:03:26.457478Z","iopub.status.idle":"2022-02-09T01:03:26.810975Z","shell.execute_reply.started":"2022-02-09T01:03:26.457444Z","shell.execute_reply":"2022-02-09T01:03:26.810127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create our posterior\nparam_posterior_a = param_k + param_prior_a\nparam_posterior_b = param_n - param_k + param_prior_b\n\n# now compute the probability using the fact that the posterior probability is a beta distribution\nP_beta_prior_formula= beta.pdf(x=x, a=param_posterior_a, b=param_posterior_b)\n\nax = sns.lineplot(x, P)\nax = sns.lineplot(x, P_beta_prior)\nax = sns.lineplot(x, P_beta_prior_formula, style=True, dashes=[(4,4)])\nax.set(xlabel='x', ylabel='f(x)', title=f'Probability Density Function for p (continuous distribution)');\nplt.legend(labels=['P, Flat Prior', 'P, Conjugate Beta Prior', 'P, Conjugate Beta Prior (formula)']);","metadata":{"execution":{"iopub.status.busy":"2022-02-09T01:06:00.162469Z","iopub.execute_input":"2022-02-09T01:06:00.162996Z","iopub.status.idle":"2022-02-09T01:06:00.664062Z","shell.execute_reply.started":"2022-02-09T01:06:00.162962Z","shell.execute_reply":"2022-02-09T01:06:00.66279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The primary benefit with conjugate priors is that we get analytic formular for the posterior distribution.  We could even create a function that allows a user to input paramter values, and outputs a posterior using a formula.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import binom\nfrom scipy.stats import beta\n\n#kwargs is keyword arguments\n\ndef posterior_from_conjugate_prior(**kwargs):\n    if kwargs['Likelihood_Dist_Type'] == 'Binomial':\n        # Get the parameters for the likelihood and prior distribution from the key word arguments.\n        x = kwargs['x'] # This is state space of possible values for p = 'probability of success' in [0,1]\n        n = kwargs['n'] # This is the number of Bernoili trials.\n        k = kwargs['k'] # This is the number of 'successes'.\n        a = kwargs['a'] # This is the parameter alpha for the prior Beta distribution\n        b = kwargs['b'] # This is the parameter beta for the prior Beta distribution\n        \n        print(f'a_prime = {k + a}.')\n        print(f'b_prime = {n - k + b}.')\n        Likelihood = binom.pmf(p=x, n=n, k=k)\n        Prior = beta.pdf(x=x, a=a, b=b)\n        Posterior = beta.pdf(x=x, a=k+a, b=n-k+b)\n        \n        return [Prior, Likelihood, Posterior]\n                    \n    else:\n        print('Distribution type not supported.')    \n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T01:07:35.907771Z","iopub.execute_input":"2022-02-09T01:07:35.908269Z","iopub.status.idle":"2022-02-09T01:07:35.918533Z","shell.execute_reply.started":"2022-02-09T01:07:35.908218Z","shell.execute_reply":"2022-02-09T01:07:35.917732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(0, 1, 0.01)\nPrior, Likelihood, Posterior = posterior_from_conjugate_prior(\n    Likelihood_Dist_Type='Binomial', \n    x=x, \n    n=50, \n    k=21, \n    a=1, \n    b=20)    \n\nax1 = sns.lineplot(x, Prior, color='red')\nax1.set(xlabel='x', ylabel='f(x)', title=f'Prior PDF');\nplt.legend(labels=['Prior PDF']);\nplt.show()\n\nax2 = sns.lineplot(x, Likelihood)\nax2.set(xlabel='x', ylabel='f(x)', title=f'Likelihood Function');\nplt.legend(labels=['Likelihood Function']);\nplt.show()\n\nax3 = sns.lineplot(x, Posterior, color='orange')\nax3.set(xlabel='x', ylabel='f(x)', title=f'Posterior PDF');\nplt.legend(labels=['Posterior PDF']);\nplt.show()\n\nax4 = sns.lineplot(x, Likelihood, color='blue')\nax4 = sns.lineplot(x, Posterior*np.mean(Likelihood)/np.mean(Posterior), color='orange')\nax4.set(xlabel='x', ylabel='f(x)', title=f'Posterior and Likelihood PDF');\nplt.legend(labels=['Likelihood PDF','Posterior PDF']);\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T01:13:00.743566Z","iopub.execute_input":"2022-02-09T01:13:00.743924Z","iopub.status.idle":"2022-02-09T01:13:02.022326Z","shell.execute_reply.started":"2022-02-09T01:13:00.743896Z","shell.execute_reply":"2022-02-09T01:13:02.021489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. CHALLENGE QUESTION\n\nModify the code from Section 5 to and add tha ability to use the posterior_from_conjugate_prior function to outout the posterior probability parameters given parameters and for a Guassian Likelihood with known variance $\\sigma^2$, and create the Prior, Likelihood, Posterior plots.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import binom\nfrom scipy.stats import beta\nfrom scipy.stats import norm\n\ndef posterior_from_conjugate_prior(**kwargs):\n    if kwargs['Likelihood_Dist_Type'] == 'Binomial':\n        # Get the parameters for the likelihood and prior distribution from the key word arguments.\n        x = kwargs['x'] # This is state space of possible values for p = 'probability of success' in [0,1]\n        n = kwargs['n'] # This is the number of Bernoili trials.\n        k = kwargs['k'] # This is the number of 'successes'.\n        a = kwargs['a'] # This is the parameter alpha for the prior Beta distribution\n        b = kwargs['b'] # This is the parameter beta for the prior Beta distribution\n        \n        print(f'a_prime = {k + a}.')\n        print(f'b_prime = {n - k + b}.')\n        Likelihood = binom.pmf(p=x, n=n, k=k)\n        Prior = beta.pdf(x=x, a=a, b=b)\n        Posterior = beta.pdf(x=x, a=k+a, b=n-k+b)\n        \n        return [Prior, Likelihood, Posterior]\n    \n    elif kwargs['Likelihood_Dist_Type'] == 'Gaussian_Known_Variance':\n        # Get the parameters for the likelihood and prior distribution from the key word arguments.\n        ### Answer the question involves completing the code in this elif routine\n        \n\n        Likelihood = -1\n        Prior = -1\n        Posterior = -1\n        \n        return [Prior, Likelihood, Posterior]\n    \n    else:\n        print('Distribution type not supported.') \n        return -1, -1, -1\n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-23T14:13:32.865753Z","iopub.execute_input":"2021-09-23T14:13:32.8663Z","iopub.status.idle":"2021-09-23T14:13:33.607029Z","shell.execute_reply.started":"2021-09-23T14:13:32.866253Z","shell.execute_reply":"2021-09-23T14:13:33.605941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(-100, 200, 0.01)\nPrior, Likelihood, Posterior = posterior_from_conjugate_prior(Likelihood_Dist_Type='Gaussian_Known_Variance', x=x, mu=50, var=21, n=2000, prior_mu=0, prior_var=1)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax1 = sns.lineplot(x, Prior, color='red')\nax1.set(xlabel='x', ylabel='f(x)', title=f'Prior PDF');\nplt.legend(labels=['Prior PDF']);\nplt.show()\n\nax2 = sns.lineplot(x, Likelihood)\nax2.set(xlabel='x', ylabel='f(x)', title=f'Likelihood Function');\nplt.legend(labels=['Likelihood Function']);\nplt.show()\n\nax3 = sns.lineplot(x, Posterior, color='orange')\nax3.set(xlabel='x', ylabel='f(x)', title=f'Posterior PDF');\nplt.legend(labels=['Posterior PDF']);\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import norm\nprint(norm.pdf(5.1, 3, 2))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T04:31:38.941443Z","iopub.execute_input":"2022-02-15T04:31:38.942057Z","iopub.status.idle":"2022-02-15T04:31:38.947336Z","shell.execute_reply.started":"2022-02-15T04:31:38.942018Z","shell.execute_reply":"2022-02-15T04:31:38.946567Z"},"trusted":true},"execution_count":null,"outputs":[]}]}